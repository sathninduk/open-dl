{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classification",
   "id": "3bebcbbda624bf24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Motivation",
   "id": "221a8a3de406306d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Classification](./images/classification_motivation.png)",
   "id": "64095f873ce1b8db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "747accc3e0981ebb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T00:03:51.917197Z",
     "start_time": "2025-10-21T00:03:42.295499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# --- 1. DATA GENERATION (Replicating the Visualization's Dataset) ---\n",
    "\n",
    "# Generate the concentric circles dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.0, factor=0.5, random_state=42)\n",
    "\n",
    "# Add the non-linear features used in the visualization to the input (Feature Engineering)\n",
    "X_1_sq = X[:, 0:1] ** 2\n",
    "X_2_sq = X[:, 1:2] ** 2\n",
    "X_1_X_2 = (X[:, 0:1] * X[:, 1:2])\n",
    "sin_X_1 = np.sin(X[:, 0:1])\n",
    "sin_X_2 = np.sin(X[:, 1:2])\n",
    "\n",
    "# Combine all features: [X1, X2, X1^2, X2^2, X1*X2, sin(X1), sin(X2)]\n",
    "X_engineered = np.hstack((X, X_1_sq, X_2_sq, X_1_X_2, sin_X_1, sin_X_2))\n",
    "\n",
    "# Split data (Visualization uses 50% test split, but we'll use 80/20 standard)\n",
    "# If you want 50/50, change test_size to 0.5\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. MODEL ARCHITECTURE (Replicating the Visualization's Structure) ---\n",
    "\n",
    "# Input: 7 features\n",
    "# Hidden Layer 1: 4 neurons, Tanh\n",
    "# Hidden Layer 2: 2 neurons, Tanh\n",
    "# Output Layer: 1 neuron, Sigmoid (for binary classification probability)\n",
    "\n",
    "# Initialize a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer (implicitly handled) and First Hidden Layer\n",
    "model.add(Dense(\n",
    "    units=4,                        # 4 neurons\n",
    "    activation='tanh',              # Tanh activation\n",
    "    input_shape=(X_engineered.shape[1],) # 7 input features\n",
    "))\n",
    "\n",
    "# Second Hidden Layer\n",
    "model.add(Dense(\n",
    "    units=2,                        # 2 neurons\n",
    "    activation='tanh'               # Tanh activation\n",
    "))\n",
    "\n",
    "# Output Layer for Binary Classification\n",
    "# Sigmoid outputs a probability (0 to 1) for the positive class\n",
    "model.add(Dense(\n",
    "    units=1,                        # 1 neuron\n",
    "    activation='sigmoid'\n",
    "))\n",
    "\n",
    "# --- 3. COMPILATION and TRAINING ---\n",
    "\n",
    "# Loss Function: Binary Cross-Entropy (standard for binary classification)\n",
    "# Optimizer: Adam, with the specified Learning Rate\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.03), # Matching the Learning Rate: 0.03\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model (for 100 epochs, similar to the 1360 in the image, to achieve high accuracy)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10, # Matching the Batch size: 10\n",
    "    verbose=0,     # Suppress training output for brevity\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# --- 4. EVALUATION (Checking the loss) ---\n",
    "\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Model trained with {X_engineered.shape[1]} engineered features.\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f} | Training Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_acc:.4f} | Test Loss:     {test_loss:.4f}\")\n",
    "\n",
    "# The model should achieve 1.0000 accuracy and nearly 0.0000 loss,\n",
    "# perfectly replicating the result in the screenshot, thanks to the\n",
    "# engineered features."
   ],
   "id": "a75637348904f69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathnindu/Develop/SLIIT/year-4/dl/DL-Lab-01/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with 7 engineered features.\n",
      "Training Accuracy: 1.0000 | Training Loss: 0.0000\n",
      "Test Accuracy:     1.0000 | Test Loss:     0.0000\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
